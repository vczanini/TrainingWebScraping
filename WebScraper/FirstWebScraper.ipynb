{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing My First Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib is a standard Python library that already contains functions to request data across the internet, it will handle cookies and change metadata such as headers and your user agent. (https://docs.python.org/3/library/urllib.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urlopen is used to open a remote object across a network and read it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to read the values in the file page1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup tries to make sense of the nonsensical; it helps format and organize web files by fixing HTML and presenting python objects representing XML structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! By convention, there is only one h1 tag on a page, but we can't count on that, so .h1 will only bring the first one (important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can think that BS reads in this way as standard\n",
    "```\n",
    "html\n",
    "├── head\n",
    "│   └── title → <title>A Useful Page</title>\n",
    "└── body\n",
    "    ├── h1 → <h1>An Interesting Title</h1>\n",
    "    └── div → <div>Lorem Ipsum dolor...</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we could call it any of those ways and it would work:  \n",
    "`bs.html.body.h1`  \n",
    "`bs.body.h1`  \n",
    "`bs.html.h1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that a lot of unexpected events can happen during a data scraping we need to be prepared for data malformation, sites offline and etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be Found\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"The Server returned an HTTP error!\")\n",
    "except URLError as e:\n",
    "    print(\"The server could not be Found\")\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
